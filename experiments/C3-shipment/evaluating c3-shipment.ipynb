{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81d0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/moon/Documents/text_to_sql_byLLM/experiments/C3-shipment/../..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "load_dotenv()\n",
    "\n",
    "experiment_path = '../..'\n",
    "\n",
    "path = os.path.abspath('')\n",
    "module_path = os.path.join(path, experiment_path)\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/functions\")\n",
    "\n",
    "\n",
    "from dataset_utils import DatasetEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f789bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH = \"results/\"\n",
    "EXPERIMENT = \"C3_SHIPMENT\"\n",
    "TARGET_PATH = f\"evaluator_results/{EXPERIMENT}_eval.csv\" \n",
    "TARGET_PATH_FK = f\"evaluator_results/{EXPERIMENT}_foreign_key_results.csv\"\n",
    "result_file_path = \"results/c3_shipment_results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c450ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_queries(queries, FILE_NAME_PATH):\n",
    "    data = {\"queries\":queries}\n",
    "    with open(FILE_NAME_PATH, \"w\") as arquivo_json:\n",
    "        json.dump(data, arquivo_json, indent=4) \n",
    "\n",
    "def read_queries(FILE_NAME_PATH):\n",
    "    with open(FILE_NAME_PATH, encoding=\"utf-8\", errors=\"ignore\") as json_data:\n",
    "        data = json.load(json_data, strict=False)\n",
    "    queries = data[\"queries\"]\n",
    "    return queries\n",
    "\n",
    "def get_schema(FILE_NAME_PATH):\n",
    "    if \"shipment\" in FILE_NAME_PATH:\n",
    "        return \"shipment\"\n",
    "    elif \"mondial_gpt\" in FILE_NAME_PATH:\n",
    "        return \"mondial_gpt\"\n",
    "    elif \"mondial\" in FILE_NAME_PATH:\n",
    "        return \"mondial\"\n",
    "    return \"\"\n",
    "\n",
    "def get_count_query_type(queries):\n",
    "    count_types = {'simple': 0, 'medium': 0, 'complex': 0}\n",
    "    for item in queries:\n",
    "        type = item[\"type\"]\n",
    "        if type in count_types:\n",
    "            count_types[type] += 1\n",
    "        else:\n",
    "            count_types[type] = 1\n",
    "    count_types['total'] = len(queries)\n",
    "    return count_types\n",
    "\n",
    "def get_dataset_evaluator(FILE_NAME_PATH):\n",
    "    \n",
    "    dataset_eval = DatasetEvaluator(dataset_file_path=f\"../../datasets/shipment/shipment_dataset.json\",\n",
    "                                dataset_tables_path=f\"../../datasets/shipment/result_tables/\",\n",
    "                                db_connection_file=f\"../../datasets/shipment_db_connection.json\",\n",
    "                                dataset_name='\"shipment\"',)\n",
    "    return dataset_eval\n",
    "\n",
    "def get_files_in_folder():\n",
    "    files = []\n",
    "    for r, d, f in os.walk(DEFAULT_PATH):\n",
    "        for file in f:\n",
    "            if '.json' in file and 'erro' not in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "    return sorted(files)\n",
    "\n",
    "def evaluate_query(queries, dataset_eval, FILE_NAME_PATH):\n",
    "    result = {'simple': 0, 'medium': 0, 'complex': 0, 'total': 0}\n",
    "\n",
    "    for q in queries:\n",
    "        evaluate_result = False\n",
    "        q['similarity'] = 0\n",
    "        try:\n",
    "            if q[\"query_string\"].strip() != \"\":\n",
    "                evaluate_result, similarity, column_matching_index = dataset_eval.evaluate_dataset_query(q[\"query_string\"], q[\"id\"], query_type=\"sql\")\n",
    "                q['column_matching_index'] = column_matching_index\n",
    "                q['similarity']=similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "        q['result'] = evaluate_result\n",
    "        \n",
    "        if evaluate_result==True:\n",
    "            result['total'] += 1\n",
    "            result[q[\"type\"]] += 1\n",
    "        \n",
    "    save_queries(queries, FILE_NAME_PATH)   \n",
    "    return result\n",
    "\n",
    "def get_file_name(FILE_NAME_PATH):\n",
    "    parts = FILE_NAME_PATH.split(\"/\")\n",
    "    last_element = parts[-1]\n",
    "    file_name = last_element.replace(\".json\", \"\")\n",
    "    return file_name\n",
    "\n",
    "def get_experiment_name(filename):\n",
    "    schema = get_schema(filename).upper()\n",
    "    experiment_name = f\"{EXPERIMENT} - {schema}\"\n",
    "    if \"chatgpt4\" in filename:\n",
    "        experiment_name += \" - GPT-4\"\n",
    "    elif \"chatgpt_\" in filename:\n",
    "        experiment_name += \" - GPT-3.5\"\n",
    "    elif \"llama\" in filename:\n",
    "        experiment_name += \" - LLAMA\"\n",
    "    \n",
    "    if \"turbo\" in filename:\n",
    "        experiment_name += \" - Turbo\"\n",
    "        \n",
    "    if \"instances\" in filename:\n",
    "        experiment_name += \" - Passing Database Rows\"\n",
    "    elif \"code_representation\" in filename:\n",
    "        experiment_name += \" - Code Representation\"\n",
    "    if \"_fk\" in filename:\n",
    "        experiment_name += \" - FK\"       \n",
    "    return experiment_name\n",
    "\n",
    "def compute_cost(filename):\n",
    "    queries = read_queries(filename)\n",
    "    total_costs = 0\n",
    "    total_tokens = 0\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    #total_token\n",
    "    for q in queries:\n",
    "        data = q.get('token_usage', None)\n",
    "        if data is not None:\n",
    "            for key, track_token in data.items():\n",
    "                total_tokens += track_token.get('total_tokens', 0)\n",
    "                total_costs += track_token.get('total_cost', 0)\n",
    "                input_tokens += track_token.get('prompt_tokens', 0)\n",
    "                output_tokens += track_token.get('completion_tokens', 0)\n",
    "    print(total_tokens)\n",
    "    print(round(total_costs, 2))\n",
    "\n",
    "    return total_tokens, round(total_costs, 2), input_tokens, output_tokens\n",
    "\n",
    "def compute_cost_specific(filename, specific_trackin):\n",
    "    queries = read_queries(filename)\n",
    "    specific_info = 0\n",
    "    for q in queries:\n",
    "        data = q.get('token_usage', None)\n",
    "        if data is not None:\n",
    "            for key, track_token in data.items():\n",
    "                specific_info += track_token.get(specific_trackin, 0)\n",
    "    return specific_info\n",
    "\n",
    "def compute_time(filename):\n",
    "    queries = read_queries(filename)\n",
    "    total_time = 0\n",
    "    for q in queries:\n",
    "        total_time += q.get('time', 0)\n",
    "    print(total_time)\n",
    "    return str(datetime.timedelta(seconds=round(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266e06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(result_files = [], results = [], compute_costs = True):\n",
    "    \n",
    "    if result_files == [] :\n",
    "        result_files = get_files_in_folder()\n",
    "    else:\n",
    "        result_files = sorted(result_files)\n",
    "        \n",
    "    \n",
    "    for file in result_files:\n",
    "        queries = read_queries(file)\n",
    "        dataset_eval = get_dataset_evaluator(file)\n",
    "        count_query_type = get_count_query_type(queries)\n",
    "        result = evaluate_query(queries, dataset_eval, file)\n",
    "        \n",
    "        overall_by_type = {}\n",
    "        for item in result:\n",
    "            overall_by_type[f\"overall_{item}\"] = round(result[item] / count_query_type[item], 4)\n",
    "        \n",
    "        result['experiment'] = get_file_name(file)\n",
    "        result = {**result, **overall_by_type}\n",
    "        \n",
    "        result['total_tokens'], result['total_cost'], result['average_tokens_by_query'] = 0, 0, 0\n",
    "        if compute_costs:\n",
    "            result['total_tokens'], result['total_cost'], result['input_tokens'], result['output_tokens'] = compute_cost(file)\n",
    "        result['total_time'] = compute_time(file)\n",
    "        results.append(result)\n",
    "        time.sleep(5)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e784b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost('results/c3_shipment_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc59384c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost_specific('results/c3_shipment_results.json', 'prompt_tokens') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3926345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost_specific('results/c3_shipment_results.json', 'completion_tokens')\n",
    "\n",
    "#Testing\n",
    "\n",
    "# with open('../../datasets/shipment/shipment_dataset.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     for q in data['queries']:\n",
    "#         if 'query_string' not in q:\n",
    "#             print(f\"Missing query_string in query ID: {q.get('id', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5bb625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DB_USER_NAME': 'SHIPMENT', 'DB_PASS': 'oraclee', 'DB_HOST': 'localhost', 'DB_PORT': 1521, 'DB_NAME': 'XE', 'SQL_DRIVER': 'oracle+oracledb', 'SERVICE_NAME': 'XE', 'SCHEMA': 'SHIPMENT', 'KEYWORD_SEARCH_API_URL': ''}\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n",
      "'query'\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results_fk = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresults/c3_shipment_results.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(result_files, results, compute_costs)\u001b[39m\n\u001b[32m     15\u001b[39m overall_by_type = {}\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     overall_by_type[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33moverall_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mround\u001b[39m(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount_query_type\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m, \u001b[32m4\u001b[39m)\n\u001b[32m     19\u001b[39m result[\u001b[33m'\u001b[39m\u001b[33mexperiment\u001b[39m\u001b[33m'\u001b[39m] = get_file_name(file)\n\u001b[32m     20\u001b[39m result = {**result, **overall_by_type}\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "results_fk = run([\n",
    " 'results/c3_shipment_results.json'], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd68a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fk = pd.DataFrame(results_fk, columns=['experiment', 'simple', 'medium', 'complex', 'total', 'overall_simple', 'overall_medium', 'overall_complex', 'overall_total','input_tokens', 'output_tokens', 'total_tokens','total_cost', 'total_time'])\n",
    "df_fk['experiment'] = df_fk['experiment'].apply(get_experiment_name)\n",
    "df_fk = df_fk.sort_values(by='overall_total', ascending=False)\n",
    "df_fk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fk.to_csv(TARGET_PATH_FK)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
