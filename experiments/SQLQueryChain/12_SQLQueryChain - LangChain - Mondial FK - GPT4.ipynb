{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLQueryChain - Mondial - GPT 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from urllib.parse import quote  \n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "load_dotenv()\n",
    "\n",
    "experiment_path = '../..'\n",
    "path = os.path.abspath('')\n",
    "module_path = os.path.join(path, experiment_path)\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/functions\")\n",
    "\n",
    "\n",
    "from sqldatabase_langchain_utils import SQLDatabaseLangchainUtils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCHEMA = 'mondial_gpt'\n",
    "# PREFIX = 'mondial'\n",
    "\n",
    "# FILE_NAME_RESULT = f\"results/12_sql_queries_chatgpt4_{SCHEMA}_fk.json\"\n",
    "\n",
    "SCHEMA = 'shipment'\n",
    "PREFIX = 'shipment'\n",
    "FILE_NAME_RESULT = f\"results/12_sql_queries_gpt4_shipment.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_queries(queries):\n",
    "    data = {\"queries\":queries}\n",
    "    with open(FILE_NAME_RESULT, \"w\") as arquivo_json:\n",
    "        json.dump(data, arquivo_json, indent=4) \n",
    "        \n",
    "def read_queries():\n",
    "    with open(FILE_NAME_RESULT, encoding='utf-8', errors='ignore') as json_data:\n",
    "        data = json.load(json_data, strict=False)\n",
    "    queries = data[\"queries\"]\n",
    "    return queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = f\"../../datasets/{SCHEMA}_db_connection.json\"\n",
    "with open(json_file_path, encoding='utf-8', errors='ignore') as json_data:\n",
    "    db_connection = json.load(json_data, strict=False)\n",
    "\n",
    "db_connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando o SQLDatabase para pegar todas as informações do database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabaseLangchainUtils(db_connection=db_connection)\n",
    "\n",
    "# exclusao = [\n",
    "#     f\"{SCHEMA}_tmdp\",\n",
    "#     f\"{SCHEMA}_tmdpmap\",\n",
    "#     f\"{SCHEMA}_tmds\",\n",
    "#     f\"{SCHEMA}_tmjmap\",\n",
    "#     f\"{SCHEMA}_tpv\",\n",
    "#     f\"{SCHEMA}_tmdc\",\n",
    "#     f\"{SCHEMA}_tmdcmap\",\n",
    "#     f\"{SCHEMA}_tmdej\",\n",
    "#     f\"{SCHEMA}_log_action\",\n",
    "#     f\"{SCHEMA}_log_error\",\n",
    "#     f\"{SCHEMA}_favorite_item\", \n",
    "#     f\"{SCHEMA}_favorite_query\",\n",
    "#     f\"{SCHEMA}_favorite_tag\",\n",
    "#     f\"{SCHEMA}_favorite_tag_item\",\n",
    "#     f\"{SCHEMA}_favorite_visualization\",\n",
    "#     f\"{SCHEMA}_dashboard\",\n",
    "#     f\"{SCHEMA}_history\",\n",
    "#     \"teste_cliente\",\n",
    "#     \"teste_fornecedor\",\n",
    "#     \"teste_funcionario\"\n",
    "# ]\n",
    "\n",
    "include_tables = db.get_table_names()\n",
    "\n",
    "# include_tables = [s for s in db.get_table_names() if not s.startswith(PREFIX) and s not in exclusao]\n",
    "db = SQLDatabaseLangchainUtils(db_connection=db_connection, include_tables=include_tables)\n",
    "db.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(db.get_table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "f = open(f\"prompts/prompt_template_sql_query_chain.txt\", \"r\")\n",
    "prompt_template = f.read()\n",
    "f.close()\n",
    "\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"], template=prompt_template\n",
    ")\n",
    "\n",
    "print(PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Chain para gerar SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_chain  = create_sql_query_chain(ChatOpenAI(temperature=0, model_name='gpt-4'), db.db, prompt=PROMPT)\n",
    "query_chain \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando as consultas em linguagem natural para rodar no LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_file_path = f\"../../datasets/{PREFIX}/queries_{PREFIX}.json\"\n",
    "with open(json_file_path, encoding='utf-8', errors='ignore') as json_data:\n",
    "    queries = json.load(json_data, strict=False)\n",
    "queries = queries['queries']\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rodando as consultas no LLM para gerar SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Every time you consult it, there will be a 10s delay to avoid API blockage.\n",
    "\n",
    "count = 0\n",
    "\n",
    "for instance in queries:\n",
    "    with get_openai_callback() as cb:\n",
    "        start_time = time.time()\n",
    "        sql_query = query_chain.invoke({\"question\":instance[\"question\"]})\n",
    "        # this uses PROMPT template by filling it with input, table_info, top_k (possibly 0), sends filled prompt to GPT 4 and gets back SQL query\n",
    "        end_time = time.time()\n",
    "        instance[\"query_string\"] = sql_query.replace('\\n', ' ').strip()\n",
    "        instance['total_tokens'] = cb.total_tokens\n",
    "        instance['prompt_tokens'] = cb.prompt_tokens\n",
    "        instance['completion_tokens'] = cb.completion_tokens\n",
    "        instance['total_cost'] = cb.total_cost\n",
    "        instance['time'] = end_time - start_time\n",
    "        print(instance['id'], instance['question'], sql_query, instance['time'], instance['total_cost'])\n",
    "    save_queries(queries)\n",
    "    time.sleep(2)\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Gerado pelo Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_query_chain_prompt = query_chain.middle[0].template.format(table_info=db.get_table_info(), top_k=0, input=\"{input}\")\n",
    "# New langChain version returns RunnableSequence instead of Chain object, doesnt allow the above method\n",
    "\n",
    "# This gives an idea of what the prompt would have looked like when sending to GPT-4\n",
    "sql_query_chain_prompt = PROMPT.format(\n",
    "    table_info=db.get_table_info(), \n",
    "    top_k=0, \n",
    "    input=\"{input}\"\n",
    ")\n",
    "print(sql_query_chain_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fix = [40,59,62,72,85,99]\n",
    "for pos in to_fix:\n",
    "    instance = queries[pos]\n",
    "    q = read_queries()\n",
    "    with get_openai_callback() as cb:\n",
    "            start_time = time.time()\n",
    "            sql_query = query_chain.invoke({\"question\":instance[\"question\"]})\n",
    "            end_time = time.time()\n",
    "            instance[\"query_string\"] = sql_query\n",
    "            instance['total_tokens'] = cb.total_tokens\n",
    "            instance['prompt_tokens'] = cb.prompt_tokens\n",
    "            instance['completion_tokens'] = cb.completion_tokens\n",
    "            instance['total_cost'] = cb.total_cost\n",
    "            instance['time'] = end_time - start_time\n",
    "            q[pos] = instance\n",
    "            print(instance['id'], instance['question'], instance[\"query_string\"], instance['time'], instance['total_cost'])\n",
    "            save_queries(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
